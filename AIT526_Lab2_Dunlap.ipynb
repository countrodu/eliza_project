{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d149cf0-1cf6-4356-b69b-bf629a3ee407",
   "metadata": {},
   "source": [
    "# Individual Lab2 - Text Summarization for Webpage\n",
    "**Course:** AIT526 Fall 2021<br>\n",
    "**Professor:** Dr. Liao<br>\n",
    "**Student:** Robb Dunlap<br>\n",
    "**Date:** September 25, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e398e-cd10-45bb-8c2f-53f9ee09db7a",
   "metadata": {},
   "source": [
    "## Task 1 (5 points) Text Summarization with Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996161cc-4572-4b18-a5cb-a113ccb35326",
   "metadata": {},
   "source": [
    "### 1.1 (0 points) \n",
    "Use the web scraping technique with BeautifulSoup as shown in class to get the text data from the specified data location on the Wikipedia webpage. Hints: Please see code snippets for web scraping in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e286d-e682-4099-8a78-15499ead2066",
   "metadata": {},
   "source": [
    "Text data location: https://en.wikipedia.org/wiki/Natural_language_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "4a26b07a-82bc-444b-9ead-c236f73a5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "eb449025-3bc8-4855-b428-8d803c4ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages that need to be loaded into the virtual environment only once (if they haven't been run before)\n",
    "# nltk.download('punkt') # this is necessary if it's the first time using word_tokenizer in your environment\n",
    "# nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e0e2f780-8bc8-40d2-93fd-0e2597b2022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet from https://towardsdatascience.com/using-beautifulsoup-on-wikipedia-dd0c620d5861\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "page = requests.get(url)\n",
    "\n",
    "# I chose to use 'html.parser' instead of 'lxml' because the html in the article is well\n",
    "# formed and I didn't want to install the lxml library for this project\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Save the requests object for testing so I don't have to keep scraping it from Wikipedia\n",
    "# Modified code snippet from https://www.studytonight.com/post/how-to-scrape-content-from-a-website-using-beautifulsoup-python\n",
    "\n",
    "with open('canned_soup', 'w', encoding='utf-8') as file:\n",
    "    for line in soup_direct:\n",
    "        file.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c6e09b-811e-41d9-8ecd-72dc6e0836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing gathering only the text from the wikipedia article\n",
    "\n",
    "def wiki_article_processor(canned_soup_file):\n",
    "    \n",
    "    # Selecting just the content between paragraph markers (<p></p>) results in just having text but it unfortunately\n",
    "    # much of the important content is structured as list markers (<li></li>). Selecting both paragraph and list markers\n",
    "    # brings in the missing text but that results in a bunch of additional text that we don't want in the analysis \n",
    "    # (for example, the TOC and the references). To solve this, I deleted these items from the Beautiful Soup object \n",
    "    # (canned_soup_file) before extracting the text from the paragraph and list sections.\n",
    "\n",
    "    # inspiration from https://medium.datadriveninvestor.com/web-scraping-wikipedia-with-beautifulsoup-a5d5fe6454ee, \n",
    "    # https://stackoverflow.com/questions/26507463/using-beautifulsoup-how-do-i-remove-a-single-class-from-an-element-with-multiple, \n",
    "    # https://kaijento.github.io/2017/03/30/beautifulsoup-removing-tags/, and \n",
    "    # https://beautiful-soup-4.readthedocs.io/en/latest/#parsing-only-part-of-a-document\n",
    "\n",
    "\n",
    "    # removes everything that's not part of the bodyContent div tag\n",
    "    only_bodyContent = SoupStrainer(id = 'bodyContent')\n",
    "    canned_soup_file = canned_soup_file.find(only_bodyContent)\n",
    "\n",
    "    removal_lst = [['ol',{'class','references'}],\n",
    "                   ['div',{'class', 'refbegin'}],\n",
    "                   ['div', {'class':'toc'}],\n",
    "                   ['div', {'role':'navigation'}],\n",
    "                   ['div', {'id':'catlinks'}],\n",
    "                   ['div', {'class':'div-col'}],\n",
    "                   ['sup', {'class':'reference'}]\n",
    "                  ]\n",
    "\n",
    "    for removal_target in removal_lst:\n",
    "        for paragraph in canned_soup_file.find_all(removal_target[0],removal_target[1]):\n",
    "            paragraph.decompose()\n",
    "\n",
    "\n",
    "    # create a string of all the content in the article\n",
    "    article_text = ''\n",
    "    for paragraph in canned_soup_file.find_all(['p', 'li']):\n",
    "        article_text += paragraph.text\n",
    "        article_text += \" \"\n",
    "\n",
    "\n",
    "    # Quantification of the amount of characters captured from the scrape and initial processing\n",
    "    # plus demonstration of the beginning and ending portions of the capture\n",
    "    print(f'Total number of characters scraped and gathered from the article: {len(article_text)}')\n",
    "    print('')\n",
    "    print('Start of the text captured from the article')\n",
    "    print(article_text[:300])\n",
    "    print('')\n",
    "    print('End of the text captured from the article')\n",
    "    print(article_text[-300:])\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8589d4-c6d5-48ea-b450-dcb49302b3b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this was the first version of the code to remove all the extraneous content. The above cell\n",
    "# has the improved version - don't run this block anymore - delete before submitting\n",
    "\n",
    "# Selecting just the content between paragraph markers (<p></p>) results in just having text but it unfortunately\n",
    "# much of the important content is structured as list markers (<li></li>). Selecting both paragraph and list markers\n",
    "# brings in the missing text but that results in a bunch of additional text that we don't want in the analysis \n",
    "# (for example, the TOC and the references). To solve this, I deleted these items from the Beautiful Soup object \n",
    "# (soup_from_can) before extracting the text from the paragraph and list sections.\n",
    "\n",
    "# inspiration from https://medium.datadriveninvestor.com/web-scraping-wikipedia-with-beautifulsoup-a5d5fe6454ee, \n",
    "# https://stackoverflow.com/questions/26507463/using-beautifulsoup-how-do-i-remove-a-single-class-from-an-element-with-multiple, \n",
    "# https://kaijento.github.io/2017/03/30/beautifulsoup-removing-tags/, and \n",
    "# https://beautiful-soup-4.readthedocs.io/en/latest/#parsing-only-part-of-a-document\n",
    "\n",
    "# # remves everything that's not part of the bodyContent div tag\n",
    "# only_bodyContent = SoupStrainer(id = 'bodyContent')\n",
    "# soup_from_can = soup_from_can.find(only_bodyContent)\n",
    "# print(len(str(soup_from_can)))\n",
    "\n",
    "# # deletes the \"references\" section\n",
    "# for paragraph in soup_from_can.find_all('ol', {'class':'references'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can)))\n",
    "\n",
    "# # deletes the \"further reading\" section\n",
    "# for paragraph in soup_from_can.find_all('div', {'class': 'refbegin'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can))) \n",
    "    \n",
    "# # deletes the \"table of contents\"    \n",
    "# for paragraph in soup_from_can.find_all('div', {'class':'toc'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can)))\n",
    "    \n",
    "# # deletes the \"navigation bar\" at the bottom\n",
    "# for paragraph in soup_from_can.find_all('div', {'role':'navigation'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can)))\n",
    "    \n",
    "# # deletes the \"links to other categories\" at the bottom of the page    \n",
    "# for paragraph in soup_from_can.find_all('div', {'id':'catlinks'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can)))\n",
    "    \n",
    "# # deletes the \"see also\" links section at the bottom of the article\n",
    "# for paragraph in soup_from_can.find_all('div', {'class':'div-col'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can))) \n",
    "\n",
    "# # deletes the endnote numbers from the text\n",
    "# for paragraph in soup_from_can.find_all('sup', {'class':'reference'}):\n",
    "#     paragraph.decompose()\n",
    "# print(len(str(soup_from_can)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "42657432-1f94-4deb-b136-030f77f22432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters scraped and gathered from the article: 16372\n",
      "\n",
      "Start of the text captured from the article\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a com\n",
      "\n",
      "End of the text captured from the article\n",
      "d by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Import the saved file for processing - only run from here down during development, don't keep\n",
    "# rescrapping Wikipedia\n",
    "\n",
    "with open('canned_soup') as file:\n",
    "    soup_from_can = BeautifulSoup(file, 'html.parser')\n",
    "    \n",
    "# run the function to process the web scape into desired text    \n",
    "processed_soup = wiki_article_processor(soup_from_can)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d5db0-19cb-488d-840b-da6ca2520dde",
   "metadata": {},
   "source": [
    "**Residual to be Removed**<br>\n",
    "Paragraph returns, parentheses, dashes, and quotes remain in the text, otherwise most other artificial symbols and extraneous text have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7758eb9-ce93-49d0-a589-e4de403228d2",
   "metadata": {},
   "source": [
    "### 1.2 (1 point) \n",
    "Preprocess the text data and must include tokenizing words, removing stop words and punctuation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d0067dea-61e2-414c-ab4c-e09b3a239005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a list of words, remove punctuation, remove stop words, and stem\n",
    "\n",
    "def token_lc_punc_rmstop_lem(input_lst_of_words, print_results):\n",
    "    \n",
    "    # Tokenization of the words\n",
    "    article_words = word_tokenize(input_lst_of_words)\n",
    "    \n",
    "    # Removal of punctuation from words\n",
    "\n",
    "    # this changes all letters to lowercase \n",
    "    article_words_wo_punc = [w.lower() for w in article_words if w not in punctuation]\n",
    "\n",
    "    # this strips punctuation at the start or end of a word\n",
    "    article_words_wo_punc = [w.strip(punctuation) for w in article_words_wo_punc]\n",
    "    \n",
    "    # Removal of stop words\n",
    "    sw = stopwords.words('english')\n",
    "    article_words_wo_punc_sw = [w for w in article_words_wo_punc if w not in sw]\n",
    "    article_words_wo_punc_sw = list(filter(None, article_words_wo_punc_sw))\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    #wnl = WordNetLemmatizer()\n",
    "    #article_words_wo_punc_sw_lemmed = [wnl.lemmatize(w) for w in article_words_wo_punc_sw]\n",
    "    porter = PorterStemmer()\n",
    "    article_words_wo_punc_sw_lemmed = [porter.stem(w) for w in article_words_wo_punc_sw]\n",
    "       \n",
    "    # print out results for user to see how many words were created from the input (if they pass True into the function)\n",
    "    if print_results:\n",
    "        print(f'# of words in the input with punctuation and stopwords removed, and lemmatized:\\\n",
    "        {len(article_words_wo_punc_sw_lemmed)}')\n",
    "        print('')\n",
    "        print('The first 20 words of the input string:')\n",
    "        print(article_words_wo_punc_sw_lemmed[:20])\n",
    "    \n",
    "    return article_words_wo_punc_sw_lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "54e992e1-c1c2-41a7-8ef0-151975e90c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the input with punctuation and stopwords removed, and lemmatized:        1500\n",
      "\n",
      "The first 20 words of the input string:\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz']\n"
     ]
    }
   ],
   "source": [
    "article_tokens = token_lc_punc_rmstop_lem(processed_soup, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "49391b21-d1d1-49aa-b365-eddfcfec8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words before removing the \"e.g\" and the right and left quotes:  1500\n",
      "total words after removing the \"e.g\" and the right and left quotes:  1465\n",
      "number of words removed during cleaning:  35\n"
     ]
    }
   ],
   "source": [
    "# the residual of the abbreviation \"e.g.\", and the right and left quotes remain in the list of words\n",
    "# after removing punctuation. This will manually remove them to prevent them messing up the analysis\n",
    "\n",
    "print('total words before removing the \"e.g\" and the right and left quotes: ',len(article_tokens))\n",
    "bad_words = ['e.g','”','“']\n",
    "i=0\n",
    "article_tokens_cleaned = []\n",
    "for word in article_tokens:\n",
    "    if word in bad_words:\n",
    "        i+=1\n",
    "    else:\n",
    "        article_tokens_cleaned.append(word)\n",
    "\n",
    "print('total words after removing the \"e.g\" and the right and left quotes: ',len(article_tokens_cleaned))\n",
    "print('number of words removed during cleaning: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37038f35-891c-4255-8721-0035e8eb9e3a",
   "metadata": {},
   "source": [
    "### 1.3 (1 point) \n",
    "Calculate word frequencies or weighted word frequencies Hint: it would be easy to use FreqDist() to get original word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "9675e3f7-d9be-4c0e-a34f-589ce64fd77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('languag', 35), ('system', 24), ('natur', 23), ('nlp', 22), ('learn', 20), ('process', 19), ('machin', 17), ('task', 16), ('rule', 16), ('statist', 16), ('cognit', 16), ('data', 15), ('use', 15), ('research', 14), ('algorithm', 13)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the word frequencies, list the top 15, and plot for visualization\n",
    "article_words_freq = nltk.FreqDist(article_tokens_cleaned)\n",
    "print(article_words_freq.most_common(15))\n",
    "article_words_freq.plot(15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2207a9-053a-4fed-9792-1125c20dae01",
   "metadata": {},
   "source": [
    "### 1.4 (1.5 points) \n",
    "Score the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "38c990f7-42e4-4167-a78a-8a1ff66f6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences: 88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences\n",
    "article_sentences = sent_tokenize(processed_soup)\n",
    "\n",
    "print(f'# of sentences: {len(article_sentences)}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9fdedfa1-af0a-4a81-9d6a-b0dddc20dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sublists of the tokenized words in each sentence. This keeps the words of a sentence grouped\n",
    "# together after the punctuation has been removed. List comprehension that uses the same function defined above to\n",
    "# tokenize, remove punctuation, and remove stopwords.\n",
    "\n",
    "# actually, now I'm just doing this to get word count per sentence\n",
    "sentence_words_tokens = [token_lc_punc_rmstop_lem(x, False) for x in article_sentences]\n",
    "\n",
    "# word counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "4d03f9fd-20c2-4713-b286-2c1fb7bc60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "281\n",
      "\n",
      "The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
      "138\n",
      "\n",
      "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "155\n",
      "\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sent in article_sentences:\n",
    "    i += 1\n",
    "    if i < 5:\n",
    "        print(sent)\n",
    "        print(len(sent))\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3cb0a8b6-7dec-4f47-a871-8d94cbe10493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "yup\n"
     ]
    }
   ],
   "source": [
    "print(len(sent.lower()))\n",
    "if 'likewise' in sent.lower():\n",
    "    print('yup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "460744f1-a1a9-4c9c-a243-7cad31e2f7b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/33lmw8sj3sbc1fnxmv660t480000gn/T/ipykernel_2661/1539921473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle_words_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msent_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sent_score_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mraw_sent_score_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0marticle_words_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Modified code from example at https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "\n",
    "raw_sent_score_dict = dict()\n",
    "for sent in article_sentences:\n",
    "    sent_length = len(sent)\n",
    "    sent_key = sent[:50]\n",
    "    for word_value in article_words_freq:\n",
    "        if word_value in sent: \n",
    "            if sent_key in raw_sent_score_dict:\n",
    "                raw_sent_score_dict[sent_key] += article_words_freq[word_value]\n",
    "            else:\n",
    "                raw_sent_score_dict[sent_key] = article_words_freq[word_value]\n",
    "    raw_sent_score_dict[sent_key] = raw_sent_score_dict[sent_key]/sent_length\n",
    "                \n",
    "dict(reversed(sorted(raw_sent_score_dict.items(), key=lambda item: item[1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "41c5c820-6b82-4667-91b3-ec7c2843bc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'linguistics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'interactions',\n",
       " 'computers',\n",
       " 'human',\n",
       " 'language',\n",
       " 'particular',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'process',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data']"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e1c3b3a8-79cd-4372-8805-a1d1df57390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech ': 73.0,\n",
       " 'coarse division given ': 24.333333333333332,\n",
       " 'misspelled words words accidentally omitted ': 14.6,\n",
       " 'natural language processing roots 1950s ': 14.6,\n",
       " 'example consider english word “ big ” ': 10.428571428571429,\n",
       " 'cognitive science interdisciplinary scientific stu': 10.428571428571429,\n",
       " 'containing words structures seen erroneous input e': 10.428571428571429,\n",
       " 'time first many chatterbots written e.g parry ': 10.428571428571429,\n",
       " 'following list commonly researched tasks natural l': 9.125,\n",
       " 'major drawback statistical methods require elabora': 9.125,\n",
       " 'however continue relevant contexts statistical int': 9.125,\n",
       " 'many different classes machine-learning algorithms': 9.125,\n",
       " 'research thus increasingly focused unsupervised se': 9.125,\n",
       " 'lines research continued e.g development chatterbo': 9.125,\n",
       " 'based long-standing trends field possible extrapol': 8.11111111111111,\n",
       " 'algorithms take input large set features generated': 8.11111111111111,\n",
       " '1980s 1980s early 1990s mark hey-day symbolic meth': 8.11111111111111,\n",
       " 'cognitive linguistics interdisciplinary branch lin': 7.3,\n",
       " 'though natural language processing tasks closely i': 7.3,\n",
       " '1980s natural language processing systems based co': 7.3,\n",
       " 'cognition refers mental action process acquiring k': 6.636363636363637,\n",
       " 'since neural turn statistical methods nlp research': 6.636363636363637,\n",
       " 'particular limit complexity systems based handwrit': 6.636363636363637,\n",
       " 'systems based automatically learning rules made ac': 6.636363636363637,\n",
       " 'algorithms learn data hand-annotated desired answe': 6.636363636363637,\n",
       " 'result great deal research gone methods effectivel': 6.636363636363637,\n",
       " 'authors claimed within three five years machine tr': 6.636363636363637,\n",
       " 'technology accurately extract information insights': 6.636363636363637,\n",
       " 'goal computer capable understanding contents docum': 6.636363636363637,\n",
       " 'recently ideas cognitive nlp revived approach achi': 6.083333333333333,\n",
       " 'especially age symbolic nlp area computational lin': 6.083333333333333,\n",
       " 'latest works tend use non-technical structure give': 6.083333333333333,\n",
       " 'cache language models upon many speech recognition': 6.083333333333333,\n",
       " 'important development eventually led statistical t': 6.083333333333333,\n",
       " 'using almost information human thought emotion eli': 6.083333333333333,\n",
       " '1950s georgetown experiment 1954 involved fully au': 6.083333333333333,\n",
       " 'likewise ideas cognitive nlp inherent neural model': 5.615384615384615,\n",
       " 'tasks direct real-world applications others common': 5.615384615384615,\n",
       " 'since 2015 field thus largely abandoned statistica': 5.615384615384615,\n",
       " 'however systems based handwritten rules made accur': 5.615384615384615,\n",
       " '2000s growth web increasing amounts raw unannotate': 5.615384615384615,\n",
       " '1970s 1970s many programmers began write conceptua': 5.615384615384615,\n",
       " 'little research machine translation conducted late': 5.615384615384615,\n",
       " 'used metaphorically ” tomorrow big day ” author ’ ': 5.214285714285714,\n",
       " 'automatic learning procedures make use statistical': 5.214285714285714,\n",
       " 'transformational grammar whose theoretical underpi': 5.214285714285714,\n",
       " 'starting late 1980s however revolution natural lan': 5.214285714285714,\n",
       " 'challenges natural language processing frequently ': 5.214285714285714,\n",
       " 'earliest-used machine learning algorithms decision': 4.866666666666666,\n",
       " 'generally task much difficult supervised learning ': 4.866666666666666,\n",
       " 'however systems depended corpora specifically deve': 4.866666666666666,\n",
       " 'due steady increase computational power see moore ': 4.866666666666666,\n",
       " 'since so-called statistical revolution late 1980s ': 4.5625,\n",
       " '1990s many notable early successes statistical met': 4.294117647058823,\n",
       " 'patient exceeded small knowledge base eliza might ': 4.294117647058823,\n",
       " 'used comparison “ big tree ” author intent imply t': 4.055555555555555,\n",
       " 'generally handling input gracefully handwritten ru': 4.055555555555555,\n",
       " 'intent behind usages like ” big person ” remain so': 3.8421052631578947,\n",
       " 'models advantage express relative certainty many d': 3.8421052631578947,\n",
       " 'increasingly important medicine healthcare nlp use': 3.8421052631578947,\n",
       " 'however real progress much slower alpac report 196': 3.8421052631578947,\n",
       " 'broadly speaking technical operationalization incr': 3.65,\n",
       " 'however creating data input machine-learning syste': 3.65,\n",
       " 'mathematical equation algorithms presented us pate': 3.4761904761904763,\n",
       " 'assign relative measures meaning word phrase sente': 3.4761904761904763,\n",
       " 'areas shift entailed substantial changes nlp syste': 3.4761904761904763,\n",
       " 'early days many language-processing systems design': 3.4761904761904763,\n",
       " 'models generally robust given unfamiliar input esp': 3.3181818181818183,\n",
       " 'examples margie schank 1975 sam cullingford 1978 p': 3.1739130434782608,\n",
       " 'increasingly however research focused statistical ': 3.0416666666666665,\n",
       " 'systems able take advantage existing multilingual ': 3.0416666666666665,\n",
       " 'machine-learning paradigm calls instead using stat': 2.92,\n",
       " 'however enormous amount non-annotated data availab': 2.92,\n",
       " 'natural language processing nlp subfield linguisti': 2.92,\n",
       " 'recent systems based machine-learning algorithms m': 2.8076923076923075,\n",
       " 'however part-of-speech tagging introduced use hidd': 2.607142857142857,\n",
       " '1960s notably successful natural language processi': 2.607142857142857,\n",
       " 'popular techniques include use word embeddings cap': 2.5172413793103448,\n",
       " 'premise symbolic nlp well-summarized john searle c': 2.5172413793103448,\n",
       " 'already 1950 alan turing published article titled ': 2.5172413793103448,\n",
       " 'instance term neural machine translation nmt empha': 2.433333333333333,\n",
       " '2010s representation learning deep neural network-': 2.212121212121212,\n",
       " 'focus areas time included research rule-based pars': 2.085714285714286,\n",
       " 'nevertheless approaches develop cognitive models t': 2.0277777777777777,\n",
       " 'example george lakoff offers methodology build nat': 1.9210526315789473,\n",
       " '2020 three trends among topics long-standing serie': 1.825,\n",
       " 'despite popularity machine learning nlp research s': 1.6590909090909092,\n",
       " 'increasing interest multilinguality potentially mu': 1.042857142857143}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified code from example at https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "\n",
    "raw_sent_score_dict = dict()\n",
    "for sent in sentence_words_tokens:\n",
    "    sent_length = len(sent)\n",
    "    reconstructed_sent = ''\n",
    "    for word in sent:\n",
    "        reconstructed_sent = reconstructed_sent + word + ' '\n",
    "        sent_key = reconstructed_sent[:50]\n",
    "    for word_value in article_words_freq:\n",
    "        if word_value in recon_sent: \n",
    "            if sent_key in raw_sent_score_dict:\n",
    "                raw_sent_score_dict[sent_key] += article_words_freq[word_value]\n",
    "            else:\n",
    "                raw_sent_score_dict[sent_key] = article_words_freq[word_value]\n",
    "    raw_sent_score_dict[sent_key] = raw_sent_score_dict[sent_key]/sent_length\n",
    "                \n",
    "dict(reversed(sorted(raw_sent_score_dict.items(), key=lambda item: item[1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a75c99-78df-4aa5-a371-15ce1ce70412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d7785-27a8-4be5-a669-6f61fc4c8b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a5165f-5dcf-4fd9-8492-7f582e44e2fe",
   "metadata": {},
   "source": [
    "### 1.5 (1.5 points) \n",
    "Build a summary based on percentage, sentence count, and word count. Print out all three summarized text examples.<br>\n",
    "Hints: For example, “percentage = 20%” means only 20% words are used for summarization. “sentence count = 3” means only 3 sentences with top frequencies are selected for summarization. “word count = 50” means the total number of words used for summarization is equal to or less than 50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad004c50-3134-4605-8037-8b2a403ba5d1",
   "metadata": {},
   "source": [
    "## Task 2 (4 points) Text Summarization with N-grams\n",
    "Based on Task 1, replace the word frequency method with the N-grams technique to summarize the same text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9c476-ad4b-401d-b92e-102372cd059f",
   "metadata": {},
   "source": [
    "### 2.1 (0 points) \n",
    "Follow the following code examples to generate N-grams with NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4f190-f845-41a8-9d62-c67c54666ec2",
   "metadata": {},
   "source": [
    "![cat](image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f1359-60b1-4563-8416-674b50a6708a",
   "metadata": {},
   "source": [
    "### 2.2 (4 points) \n",
    "Write the code for text summarization with any N-grams. Note that we will check your program using at least two different n-grams, e.g., n=2, 3, or 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb741e9b-930d-44b7-af9f-6b7c4af90f96",
   "metadata": {},
   "source": [
    "#### Hint 1 (0.5 points) \n",
    "Use NLTK to get N-grams and FreqDist() to calculate the n-gram frequencies. The running outputs should be similar to:\n",
    "![cat](image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761c68d-47ac-46f7-b7bc-66a0b7a5f1a6",
   "metadata": {},
   "source": [
    "#### Hint 2 (1 point) \n",
    "Find weighted frequency occurrences from FreqDist. You can use the similar function from Task 1. The code snippets and running outputs may be similar to:\n",
    "![cat](image_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb54ed-bd76-4c2a-a02e-f283be59392f",
   "metadata": {},
   "source": [
    "#### Hint 4 (1.5 point) \n",
    "Define the function like calculate_sentence_scores_ngram(sent_tokens, ngram_freqs, n_grams) to calculate the sentence scores for any N-grams. This function is similar to the one in Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a90a94-3e95-4a1c-b53b-89a06fdb1d65",
   "metadata": {},
   "source": [
    "#### Hing 4 (1 points) \n",
    "Getting the summary is the same as Task 1. The following running output examples show the summarized text using tri-grams and 3 sentences (based on percentage, sentence count, and word count). One of the summarized examples are as follows:\n",
    "![cat](image_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63325f0-1d26-43ba-a505-cfc1007cbe10",
   "metadata": {},
   "source": [
    "## Task 3 (1 point) Comparisons\n",
    "Compare these two methods for text summarization. What are different and why? Please clearly explain (100-300 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772dd1d-7793-42bb-95f1-b8cb4033162f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
