{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccf1762-4ae4-4a0b-9c81-b63479c253e2",
   "metadata": {},
   "source": [
    "## Class: AIT 526                                             September 15, 2021\n",
    "### Professor:  Dr. Duoduo (Lindi) Liao\n",
    "### Team 5: \n",
    "- Anh \"Tim\" Hien Bach\n",
    "- Robert \"Robb\" Jay Dunlap\n",
    "- Vishnu Lasya Marthala\n",
    "- David Earl Swanson\n",
    "\n",
    "#### Programming Assignment – Chatbot Eliza\n",
    "Write an Eliza program in Python. No chat packages/functions/libraries are allowed to use. The program should be called eliza.py, and it should run from the command line with no arguments. NOTE: if you use **jupyter notebook**, you can have all comments and required running outputs/logs in the notebook file(s). And then save into HTML(s) and zip all notebook file(s), HTML files, and other files into ONE zip file. Please submit only one zip file.\n",
    "\n",
    "Your program should engage in a dialogue with the user, with your program Eliza playing the role of a psychotherapist. Your program should be able carry out \"word spotting\", that is it should recognize certain key words and respond simply based on that word being present in the input. It should also be able to transform certain simple sentence forms from statements (from the user) into questions (that Eliza will ask). Also, try to personalize the dialogue by asking and using the user's name. \n",
    "\n",
    "In addition, your program should be robust. If the user inputs gibberish or a very complicated question, Eliza should respond in some plausible way (I didn't quite understand, can you say that another way, etc.). “Word spotting”, sentence transformation, and robustness are the minimum requirements for your code. You can implement additional functionalities, inspired by the dialogues presented in Weizenbaum paper. You may receive up to 1 bonus point max for any additional functionalities. \n",
    "\n",
    "This program should rely heavily on the use of regular expressions, so please make sure to review some introductory material in Learning Python, Programming Python, or some other source before attempting this program.\n",
    "\n",
    " \n",
    "\n",
    "#### Tasks\n",
    "- \n",
    "\n",
    "### Be sure to comment your code. In particular, explain what words you are spotting for (and why) and what statement forms you are converting into questions (and why). Also make sure your name, class, etc. is clearly included in the comments.\n",
    "\n",
    "Due: September 21, 2021\n",
    "\n",
    "### ELIZA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfe0ad-569b-48d6-a4f9-a822f06ff2be",
   "metadata": {},
   "source": [
    "### Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298af1e9-6331-4997-9f37-a97f2bb5bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser, RegexpChunkParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#import contractions\n",
    "\n",
    "# Using the TreebankTagger instead of the Perceptron model in NLTK (TreebankTagger is muuch better)\n",
    "# I found an explanation and the below line of code in this SO post:\n",
    "# https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag\n",
    "treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4d3f4-a5b8-4032-b2e4-516ed9ab13ae",
   "metadata": {},
   "source": [
    "### We decided to \"load\" or read in several dictionaries that support the algorithm in each function rather than hard coding them into the notebook.\n",
    "####    This section loads those dictionaries\n",
    "Code written by:  The full team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2001585-720c-488f-be97-328e2aa033d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a dictionary of contractions\n",
    "file = open(\"contraction_dictionary.txt\", \"r\")\n",
    "contents = file.read()\n",
    "contractions = ast.literal_eval(contents)\n",
    "file.close\n",
    "\n",
    "# Read in a dictionary of the emergency words\n",
    "file = open(\"emergency_words.txt\", \"r\")\n",
    "content = file.read()\n",
    "emergency_words = content.split(\",\")\n",
    "file.close\n",
    "\n",
    "# Read in the emergency response\n",
    "file = open('emergecy_response.txt', 'r', encoding = 'utf-8')\n",
    "emergency_response = file.read()\n",
    "file.close\n",
    "\n",
    "# Open the nonspecific responses that will be used when Eliza doesn't understand the patient's statement.\n",
    "file = open(\"gibberish_responses.txt\", \"r\")\n",
    "nonspecific_response = file.readlines()\n",
    "file.close\n",
    "\n",
    "# Open the list of words to be spotted and find synonyms of them to creaste a dictionary of spotting words\n",
    "file = open(\"spotted_words.txt\", \"r\")\n",
    "list_key = file.read()\n",
    "file.close\n",
    "\n",
    "dic_spotwords={}\n",
    "ListKey = list_key.split()\n",
    "for word in ListKey:\n",
    "    dup_synonyms=[]\n",
    "    for synonym in wordnet.synsets(word):\n",
    "        for lemma in synonym.lemmas():\n",
    "            dup_synonyms.append(lemma.name())\n",
    "\n",
    "    unique_synonyms=[]\n",
    "    for synonym in dup_synonyms:\n",
    "        if synonym not in unique_synonyms:\n",
    "            unique_synonyms.append(synonym)\n",
    "    dic_spotwords[word]=unique_synonyms\n",
    "\n",
    "# I don't think we use these regular expressions\n",
    "reg_exp_keywords={\n",
    "    r'.*[i|I] am (sad)':[\"Why do you think you are {0}?\"],\n",
    "    r'.*[i|I] am (dull)':[\"Why do you think you are {0}?\"] \n",
    "    # r'.*I am (depressed|sad).*':[\"Why do you think you are\" r' \\1' \"?\" ] ,\n",
    "    # r'.*I am (depressed|sad).*':[\"Why do you think you are (0) ?\" ] \n",
    "    }\n",
    "\n",
    "#These need to be greatly ezxpanded and turnedinto a file for reading\n",
    "standardResponses={\n",
    "       'hello':'Cool! Go ahead.',\n",
    "       'help':'I am here to help you with your emotions and thought process.',\n",
    "       'happy':'Awesome! What makes you happy?',\n",
    "       'gibberish':\"I'm not sure I'm following you, please explain.\"\n",
    "    }\n",
    "\n",
    "# This section loads the necessary information for the \"alternate_phrase\" function below. The csv contains:\n",
    "# (1) RE patterns for creating specific chunks from using POS labels\n",
    "# (2) Identifying the portion of the user input that will be \"turned\" back to them in the response\n",
    "# (3) The framework response phrases that the portion identifiied in (2) will be married to\n",
    "\n",
    "input_terms_df = pd.read_csv('input_chunk_terms.csv', \n",
    "                             dtype={'capture_expression':'str', \n",
    "                                     'insertion':'bool', \n",
    "                                     'response_template':'str'}\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cfa53-e65a-4ca0-818e-c057edfc8da2",
   "metadata": {},
   "source": [
    "### Functions used by the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9c3b9a-81cd-451f-a0fb-9fa671667baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code written by Vishnu Lasya Marthala and edited by David Swanson \n",
    "    \n",
    "def standard_phrase(text):\n",
    "    for token in text:    \n",
    "        for key, value in dic_spotwords.items(): \n",
    "            if token in value:\n",
    "                for key_r,value_r in standardResponses.items():\n",
    "                    if(key == key_r):\n",
    "                        return(value_r)\n",
    "    return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4292859f-0e97-459d-a08f-c748897b876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by Robb Dunlap\n",
    "# A function for counting the occurences of specific POS in a sentence - \n",
    "#his function takes in a target sentence and the a regular expression phrase, it then\n",
    "# counts the occurence of the phrase in the \n",
    "def occurence_in_sent(sentence, regex):\n",
    "    regex_of_word = re.compile(regex)\n",
    "    occ_counter = 0\n",
    "    for element in sentence:\n",
    "        if re.match(regex_of_word, element[1]):\n",
    "            occ_counter += 1\n",
    "    return occ_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0bbf50-4a1b-4b4a-8400-bad634e450fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method for parsing user input and generating a response - code written by Robb Dunlap\n",
    "\n",
    "def alternate_phrase(user_sentence):\n",
    "    \n",
    "    # declaring empty strings that will hold the individual words from the sentence and the reconstructed \n",
    "    # sentence after the contractions are expanded \n",
    "    new_sentence = \"\"\n",
    "    new_word = \"\"\n",
    "    for word in user_sentence.split():\n",
    "        for key, root in contractions.items():\n",
    "            if key == word:\n",
    "                word = root\n",
    "        new_word = word+\" \"\n",
    "        new_word += \" \"\n",
    "        # Had to put the words back together as a string instead of as a list because the two separate base words on the contraction\n",
    "        # end up being one entry in a list. So instead, reassemble the sentence as a string and split it again to be sure to get\n",
    "        # individual words\n",
    "        new_sentence += new_word\n",
    "\n",
    "    # need to strip off extra space added to the end of the sentence\n",
    "    new_sentence = new_sentence[:-2]\n",
    "\n",
    "    # create a list of the sentence words in order\n",
    "    split_sent = new_sentence.split()\n",
    "    sent_words_wo_punct = [w.strip(punctuation) for w in split_sent]\n",
    "\n",
    "\n",
    "    tokens_tag = treebankTagger.tag(sent_words_wo_punct)\n",
    "\n",
    "    # using the occurence_in_sent function to count the number of nouns or verbs\n",
    "    # if the count of either is greater than 4 then the sentence is too complicated\n",
    "    # for turning the user's input. Instead, this module will return an empyt string\n",
    "    # so the main loop can call the \"gibberish/too complicated response\"\n",
    "\n",
    "    noun_pos_regex = \"NN.?\"\n",
    "    count_of_nouns = occurence_in_sent(tokens_tag, noun_pos_regex)\n",
    "\n",
    "    verb_pos_regex = \"VB.?\"\n",
    "    count_of_verbs = occurence_in_sent(tokens_tag, verb_pos_regex)\n",
    "\n",
    "    if count_of_nouns > 4 or count_of_verbs > 4:\n",
    "        return elize_response\n",
    "\n",
    "    else:\n",
    "        index_in_df = -1\n",
    "        # for statement in chunk_list:\n",
    "        for statement in input_terms_df['capture_expression']:\n",
    "            pattern_found = 0\n",
    "            index_in_df+=1\n",
    "            phrase_pattern = statement\n",
    "            chunker = RegexpParser(phrase_pattern)\n",
    "            if index_in_df<2:\n",
    "                output = chunker.parse(tokens_tag)\n",
    "            else:\n",
    "                output = chunker.parse(output)\n",
    "\n",
    "            key_word = []\n",
    "            for item in output:\n",
    "                if isinstance(item, nltk.tree.Tree):\n",
    "                    pattern_found = 1\n",
    "                    regex_desired_word = re.compile(input_terms_df.iloc[index_in_df,3])\n",
    "                    for thing in item:\n",
    "                        if re.match(regex_desired_word, thing[1]):     \n",
    "                            key_word = thing[0]\n",
    "            if pattern_found == 1:\n",
    "                break\n",
    "\n",
    "        # the below section captures the desired portion of the sentence so it can be flipped into the response\n",
    "\n",
    "        position_in_sent_word = 0\n",
    "        counter = 0\n",
    "        for element in sent_words_wo_punct:\n",
    "            if key_word == element:\n",
    "                counter = position_in_sent_word\n",
    "            position_in_sent_word += 1\n",
    "\n",
    "        offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "        turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "\n",
    "        # This section appends or inserts the flipped portion of the input text to the appropriate template \n",
    "        if pattern_found == 1:\n",
    "            if not input_terms_df.iloc[index_in_df,1]:\n",
    "                offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "                turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "                if not turned_portion_of_sent:\n",
    "                    return elize_response\n",
    "                else:\n",
    "                    temp_holder = \"\"\n",
    "                    separator = \" \"\n",
    "                    temp_holder = separator.join(turned_portion_of_sent)\n",
    "                    question_mark = \"?\"\n",
    "                    temp_holder.join(question_mark)\n",
    "                    eliza_response = input_terms_df.iloc[index_in_df,2]+\" \"+temp_holder+\"?\"\n",
    "            else:\n",
    "                offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "                turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "                if not turned_portion_of_sent:\n",
    "                    return elize_response\n",
    "                else:\n",
    "                    temp_holder = \"\"\n",
    "                    separator = \" \"\n",
    "                    temp_holder = separator.join(turned_portion_of_sent)\n",
    "                    turned_portion_of_sent = str(temp_holder)\n",
    "                    eliza_response = input_terms_df.iloc[index_in_df,2].replace(\"XXYYMM\",turned_portion_of_sent)\n",
    "            return eliza_response    \n",
    "\n",
    "        else:\n",
    "            elize_response = \"\"\n",
    "            return elize_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da8ca8-5377-4f13-a92a-2f24231905f9",
   "metadata": {},
   "source": [
    "### Eliza welcomes the patient, sets the tone for the conversation, and establishes an end of session codeword \"quit.\"\n",
    "#### We decided that Eliza could asked for the patient's name which is used to tag the patient's inputs lines as well as a salutation.\n",
    "Code written by David Swanson and Vishnu Lasya Marthala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477ab70-acfe-41de-9f5d-c56dd2fa5926",
   "metadata": {},
   "source": [
    "**To use Spacy en_core_web_sm** I had to load add the Spacy module to my Conda environment and then I also had to use the following code in a terminal in my environment to add the library: &emsp; `conda install -c conda-forge spacy-model-en_core_web_sm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f5dac-e7a8-4160-bf0f-a8dcfd67f2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04218816-b7b4-40b5-8094-0ede935cbc6d",
   "metadata": {},
   "source": [
    "#### The following code is a loop that\n",
    "1. Tokenizes the sentances in the response\n",
    "2. If the patient talks too long (more than two sentences) Eliza asks in several ways for the patient to slow down \n",
    "3. Tokenizes the words of each sentence to be sent to four functions in sequence.\n",
    "    1. Has the patient made a statement that is dangerous to themselves of the theoropist (aka emergency words)? \n",
    "                     - If so, instruct the patient to seek help and end the session \n",
    "    2. Can Eliza respond using spotted words?\n",
    "    3. Can a sophisticated regular expression algorithm make a response?\n",
    "    4. If all these fail, Eliza assumes the answer was gibberish and randomly selects a response for the patient to talk more.\n",
    "    \n",
    "Code written by: David Swanson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb969a-7edc-4e17-8938-5318444f8135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: I don't want you to think of these sessions as therapy but rather as an opportunity for self-reflection and growth.\n",
      "Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"\n",
      "\n",
      "Eliza: What's your name?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Patient:  Rob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: Hello Rob. How are you feeling today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  I'm feeling great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza2:  What does \"feeling great\" mean to you?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  It means that I'm happy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza2:  Why do you feel happy?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  Everything is working out great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza3:  I see, please continue.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  Well, school is going swimmingly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza2:  Is \"going swimmingly\" important to you\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  Sure, I like when things are going well\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza2:  Is \"going well\" important to you\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rob:  Yes, that too is important to me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza2:  Tell me more about your statement \"too is important to me\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Eliza: Hello! My name is Eliza.  Welcome to my practice.\")\n",
    "print(\"Eliza: I don't want you to think of these sessions as therapy but rather as an opportunity for self-reflection and growth.\")\n",
    "print('Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"')\n",
    "print(\"\")\n",
    "\n",
    "# Get the patient's name\n",
    "print(\"Eliza: What's your name?\")\n",
    "print(\"\")\n",
    "user_input = input(\"Patient: \")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(user_input)  \n",
    "usernames=[e for e in doc.ents if e.label_ == 'PERSON']\n",
    "\n",
    "if doc.ents==():\n",
    "    patientName = \"Patient: \"\n",
    "    print(\"Eliza: That's ok, we don't need to use names. So how are you feeling today?\")\n",
    "else:\n",
    "    patientName=usernames[0].text\n",
    "    print('Eliza: Hello '+ patientName +'. '+\"How are you feeling today?\")\n",
    "\n",
    "    \n",
    "# Entering while loop for continuing discussion with the patient\n",
    "\n",
    "loopAgain = True\n",
    "\n",
    "# Top of the loop\n",
    "while (loopAgain == True):\n",
    "    \n",
    "    # Get the patient's input \n",
    "    print(\"\")\n",
    "    user_input = input(patientName+\": \")\n",
    "    \n",
    "# First check the patient's response to see if they want to end the session.\n",
    "    if (user_input.lower() == \"quit\"):\n",
    "        print(\"Eliza: I hope our session was helpful.  Goodbye\",patientName)\n",
    "        loopAgain = False\n",
    "        break\n",
    "        \n",
    "# Start a counter to cycle through some random responses when the patients drones on\n",
    "    lengthyReponse = 0\n",
    "    \n",
    "# Separate the response into individual sentences\n",
    "    PatientResponse = nltk.sent_tokenize(user_input)\n",
    "    # print(PatientResponse)\n",
    "    \n",
    "# Send the patient's response word by word to check for emergency words\n",
    "# Code written by Anh \"Tim\" Hien Bach\n",
    "\n",
    "    HaltFlag = 0\n",
    "    for sentences in PatientResponse:\n",
    "        PatientWords = nltk.word_tokenize(sentences)\n",
    "        for words in PatientWords: \n",
    "            for EmWord in emergency_words:\n",
    "                # If any emergency word is found, return with a standard cautionary response\n",
    "                if re.search(EmWord, words):\n",
    "                    print(patientName,\": \", emergency_response)\n",
    "                    loopAgain = False\n",
    "                    break\n",
    "            if loopAgain == False:\n",
    "                break\n",
    "        if loopAgain == False:\n",
    "            break\n",
    "\n",
    "# If the response is more than 2 sentences\n",
    "    if loopAgain == True and len(PatientResponse)>2:\n",
    "        lengthyReponse = lengthyReponse + 1\n",
    "\n",
    "# Cycle through two responses if the patient inputs 3 sentences \n",
    "        if(len(PatientResponse)==3): \n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: Let's go slower and take things one at a time. What is on your mind?\")\n",
    "            if(lengthyReponse > 1):\n",
    "                print(\"Eliza: Again, slow down. Try again.\")\n",
    "                lengthy_response = 0\n",
    "                \n",
    "# Cycle through three three responses if the patient inputs more than 3 sentences \n",
    "        else: \n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: Whoa, that's a lot to unpack. Let's try again with shorter thoughts.\")\n",
    "            if(lengthyReponse == 2):\n",
    "                print(\"Eliza: Again, slow down. Try again.\")\n",
    "            if(lengthyReponse > 2):\n",
    "                print(\"Eliza: You have alot on your mind. Let's start again with your first thought.\")\n",
    "                lengthyReponse = 0\n",
    "\n",
    "# If the patient gives one or two sentence response, address each            \n",
    "    else:\n",
    "        if loopAgain == False:\n",
    "            break\n",
    "        for sentence in PatientResponse:\n",
    "            \n",
    "            # We don't weant contractions in our evaluation of the patient's input so we expand them \n",
    "            newSentence = \"\"\n",
    "            for word in sentence.split():\n",
    "                for key, root in contractions.items():\n",
    "                    if key == word:\n",
    "                        word = root\n",
    "                    elif key == word.lower():\n",
    "                        word = root\n",
    "            \n",
    "            # Tokenize the updated sentence into words\n",
    "            tokenizer = RegexpTokenizer (r'\\w+')\n",
    "            words_only = tokenizer.tokenize(newSentence)\n",
    "            \n",
    "            # Debugging Code\n",
    "            # print (\"The total number of words in sentence\", i, \"of the user's input is:\", len(words_only)) \n",
    "            # print (words_only)\n",
    "    \n",
    "            # Set ElizaResponse to NULL\n",
    "            ElizaResponse = \"\"\n",
    "            \n",
    "            # Send the patient's sentence to a word spotter routine \n",
    "            ElizaResponse = standard_phrase(words_only)\n",
    "            if ElizaResponse != \"\":\n",
    "                print(\"Eliza1: \", ElizaResponse)\n",
    "                break\n",
    "                \n",
    "            # If the standard phrase routine fails to provide a response send to the alternate function \n",
    "            # We use the untokenized here because the function processes the phrase differently than the \n",
    "            # standard function\n",
    "            ElizaResponse = alternate_phrase(PatientResponse[0])\n",
    "            if ElizaResponse != \"\":\n",
    "                print(\"Eliza2: \", ElizaResponse)\n",
    "                break\n",
    "                    \n",
    "            # If the standard function and the alternate function aren't able to generate a response, then\n",
    "            # a random pick from the gibberish/too complicated responses is called.\n",
    "            print(\"Eliza3: \", random.choice(nonspecific_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e3250-2be1-4463-9282-eabdbabd9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatientResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d61214-d6e9-4727-ac88-963581f87d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
