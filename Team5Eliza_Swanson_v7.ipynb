{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: AIT 526                                             September 15, 2021\n",
    "### Professor:  Dr. Duoduo (Lindi) Liao\n",
    "### Team 5: \n",
    "- Anh \"Tim\" Hien Bach\n",
    "- Robert \"Robb\" Jay Dunlap\n",
    "- Vishnu Lasya Marthala\n",
    "- David Earl Swanson\n",
    "\n",
    "#### Programming Assignment – Chatbot Eliza\n",
    "Write an Eliza program in Python. No chat packages/functions/libraries are allowed to use. The program should be called eliza.py, and it should run from the command line with no arguments. NOTE: if you use **jupyter notebook**, you can have all comments and required running outputs/logs in the notebook file(s). And then save into HTML(s) and zip all notebook file(s), HTML files, and other files into ONE zip file. Please submit only one zip file.\n",
    "\n",
    "Your program should engage in a dialogue with the user, with your program Eliza playing the role of a psychotherapist. Your program should be able carry out \"word spotting\", that is it should recognize certain key words and respond simply based on that word being present in the input. It should also be able to transform certain simple sentence forms from statements (from the user) into questions (that Eliza will ask). Also, try to personalize the dialogue by asking and using the user's name. \n",
    "\n",
    "In addition, your program should be robust. If the user inputs gibberish or a very complicated question, Eliza should respond in some plausible way (I didn't quite understand, can you say that another way, etc.). “Word spotting”, sentence transformation, and robustness are the minimum requirements for your code. You can implement additional functionalities, inspired by the dialogues presented in Weizenbaum paper. You may receive up to 1 bonus point max for any additional functionalities. \n",
    "\n",
    "This program should rely heavily on the use of regular expressions, so please make sure to review some introductory material in Learning Python, Programming Python, or some other source before attempting this program.\n",
    "\n",
    " \n",
    "\n",
    "#### Tasks\n",
    "- \n",
    "\n",
    "### Be sure to comment your code. In particular, explain what words you are spotting for (and why) and what statement forms you are converting into questions (and why). Also make sure your name, class, etc. is clearly included in the comments.\n",
    "\n",
    "Due: September 21, 2021\n",
    "\n",
    "### ELIZA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Fall 2021\\\\NLP- AIT 526\\\\Programmming Assignments\\\\1. Prg Ass'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "#import txt \n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser, RegexpChunkParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#import contractions\n",
    "\n",
    "# Using the TreebankTagger instead of the Perceptron model in NLTK (TreebankTagger is muuch better)\n",
    "# I found an explanation and the below line of code in this SO post:\n",
    "\n",
    "# https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag\n",
    "treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
    "os.chdir('D:\\\\Fall 2021\\\\NLP- AIT 526\\\\Programmming Assignments\\\\1. Prg Ass')\n",
    "os.getcwd()\n",
    "#vishnu \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided to \"load\" or read in several dictionaries that support the algorithm in each function rather than hard coding them into the notebook.\n",
    "####    This section loads those dictionaries\n",
    "Code written by:  The full team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': 'Cool! Go ahead.', 'help': 'I am here to help you with your emotions and thought process.', 'yes': 'Tell me more about it.', 'happy': 'Awesome! What makes you happy?', 'good': 'Fab! What makes you feel good?', 'bad': 'Oh! But, What makes you feel bad?', 'sick': 'What makes you sick?', 'healthy': 'What makes you feel healthy?', 'worst': 'What makes you feel worst?', 'unhealthy': 'What makes you feel unhealthy?', 'best': 'What makes you feel the best?', 'bye': 'Bye!', 'discourage': 'What kind of attitude do you have? Is it Positive or Negative?', 'motiveless': 'What kind of attitude do you have? Is it Positive or Negative?', 'positive': 'Cool! Go ahead.', 'negative': 'Try to read motivational books and also listen to podcasts for positive vibes.', 'support': 'Change your thoughts and put your plans to action, and I can assure, you are going to feel much better!', 'grateful': 'Happy to help! Be Cool and Stay Positive.', 'appreciate': 'Happy to help! Be Cool and Stay Positive.', 'fallback': \"I didn't quite understand. Can you rephrase it?\"}\n",
      "{'\\\\b[i|I] am (.*)\\\\b': ['Why do you think you are {0}?'], '\\\\b[i|I] need(.*)\\\\b': ['Why do you need {0}?'], '\\\\b[i|I] feel(.*)\\\\b': ['How often do you feel {0}?'], '\\\\b[i|I] do not (.*)\\\\b': ['Would it really help if you {0}?'], '\\\\b[i|I] remember(.*)\\\\b': ['Why do you remember {0}?'], '\\\\b[i|I] know(.*)\\\\b': ['How do you know {0}?'], '\\\\b[i|I] like to (.*)\\\\b': ['From when did you start liking {0}?'], '\\\\b[i|I] hate to (.*)\\\\b': ['Why do you hate to be {0}?'], '\\\\b[i|I] crave to (.*)\\\\b': ['Why do you crave {0}?'], '\\\\b[i|I] dream (.*)\\\\b': ['Why do you dream of {0}?'], '\\\\b[i|I] love (.*)\\\\b': ['Why do you love {0}?']}\n",
      "['help', 'hello', 'happy', 'good', 'bad', 'sick', 'healthy', 'worst', 'unhealthy', 'best', 'bye', 'discourage', 'motiveless', 'positive', 'negative', 'support', 'grateful', 'appreciate', 'bye', 'greetings', 'yes']\n",
      "{'help': ['aid', 'assist', 'assistance', 'help', 'assistant', 'helper', 'supporter', 'avail', 'service', 'facilitate', 'help_oneself', 'serve'], 'hello': ['hello', 'hullo', 'hi', 'howdy', 'how-do-you-do'], 'happy': ['happy', 'felicitous', 'glad', 'well-chosen'], 'good': ['good', 'goodness', 'commodity', 'trade_good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'thoroughly', 'soundly'], 'bad': ['bad', 'badness', 'big', 'tough', 'spoiled', 'spoilt', 'regretful', 'sorry', 'uncollectible', 'risky', 'high-risk', 'speculative', 'unfit', 'unsound', 'forged', 'defective', 'badly'], 'sick': ['sick', 'vomit', 'vomit_up', 'purge', 'cast', 'cat', 'be_sick', 'disgorge', 'regorge', 'retch', 'puke', 'barf', 'spew', 'spue', 'chuck', 'upchuck', 'honk', 'regurgitate', 'throw_up', 'ill', 'nauseated', 'nauseous', 'queasy', 'sickish', 'brainsick', 'crazy', 'demented', 'disturbed', 'mad', 'unbalanced', 'unhinged', 'disgusted', 'fed_up', 'sick_of', 'tired_of', 'pale', 'pallid', 'wan', 'ghastly', 'grim', 'grisly', 'gruesome', 'macabre'], 'healthy': ['healthy', 'salubrious', 'good_for_you', 'intelligent', 'levelheaded', 'level-headed', 'sound', 'goodly', 'goodish', 'hefty', 'respectable', 'sizable', 'sizeable', 'tidy'], 'worst': ['worst', 'pip', 'mop_up', 'whip', 'rack_up', 'bad', 'big', 'tough', 'spoiled', 'spoilt', 'regretful', 'sorry', 'uncollectible', 'risky', 'high-risk', 'speculative', 'unfit', 'unsound', 'forged', 'defective'], 'unhealthy': ['unhealthy', 'insalubrious', 'unhealthful'], 'best': ['best', 'topper', 'Best', 'C._H._Best', 'Charles_Herbert_Best', 'outdo', 'outflank', 'trump', 'scoop', 'better', 'good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'easily', 'considerably', 'substantially', 'intimately', 'advantageously', 'comfortably'], 'bye': ['bye', 'pass', 'adieu', 'adios', 'arrivederci', 'auf_wiedersehen', 'au_revoir', 'bye-bye', 'cheerio', 'good-by', 'goodby', 'good-bye', 'goodbye', 'good_day', 'sayonara', 'so_long'], 'discourage': ['deter', 'discourage', 'warn', 'admonish', 'monish'], 'motiveless': ['motiveless', 'unprovoked', 'wanton'], 'positive': ['positive', 'positive_degree', 'convinced', 'confident', 'plus', 'confirming', 'prescribed', 'incontrovertible', 'irrefutable', 'positivist', 'positivistic', 'electropositive', 'positively_charged', 'cocksure', 'overconfident'], 'negative': ['negative', 'veto', 'blackball', 'disconfirming', 'damaging', 'electronegative', 'negatively_charged', 'minus'], 'support': ['support', 'reinforcement', 'reenforcement', 'documentation', 'keep', 'livelihood', 'living', 'bread_and_butter', 'sustenance', 'supporting', 'accompaniment', 'musical_accompaniment', 'backup', 'financial_support', 'funding', 'backing', 'financial_backing', 'back_up', 'back', 'endorse', 'indorse', 'plump_for', 'plunk_for', 'hold', 'sustain', 'hold_up', 'confirm', 'corroborate', 'substantiate', 'affirm', 'subscribe', 'underpin', 'bear_out', 'defend', 'fend_for', 'patronize', 'patronise', 'patronage', 'keep_going', 'digest', 'endure', 'stick_out', 'stomach', 'bear', 'stand', 'tolerate', 'brook', 'abide', 'suffer', 'put_up'], 'grateful': ['grateful', 'thankful'], 'appreciate': ['appreciate', 'take_account', 'prize', 'value', 'treasure', 'apprize', 'apprise', 'revalue'], 'greetings': ['greeting', 'salutation', 'greet', 'recognize', 'recognise'], 'yes': ['yes']}\n"
     ]
    }
   ],
   "source": [
    "# Read in a dictionary of contractions\n",
    "file = open(\"contraction_dictionary.txt\", \"r\")\n",
    "contents = file.read()\n",
    "contractions = ast.literal_eval(contents)\n",
    "file.close\n",
    "\n",
    "# Read in a dictionary of the emergency words\n",
    "file = open(\"emergency_words.txt\", \"r\")\n",
    "content = file.read()\n",
    "emergency_words = content.split(\",\")\n",
    "file.close\n",
    "\n",
    "# Read in the emergency response\n",
    "file = open('emergency_response.txt', 'r', encoding = 'utf-8')\n",
    "emergency_response = file.read()\n",
    "file.close\n",
    "\n",
    "# Open the nonspecific responses that will be used when Eliza doesn't understand the patient's statement.\n",
    "file = open(\"gibberish_responses.txt\", \"r\")\n",
    "nonspecific_response = file.readlines()\n",
    "file.close\n",
    "\n",
    "# Open the list of words to be spotted and find synonyms of them to creaste a dictionary of spotting words\n",
    "file = open(\"spotted_words.txt\", \"r\")\n",
    "list_key = file.read()\n",
    "file.close\n",
    "#vishnu\n",
    "file = open(\"Standard_Responses.txt\", \"r\")\n",
    "standardresponse = file.read()\n",
    "standard_response=ast.literal_eval(standardresponse)\n",
    "file.close\n",
    "print(standard_response)\n",
    "#print((standard_response['sad']))\n",
    "\n",
    "#vishnu\n",
    "file = open(\"reg_exp_pattern.txt\", \"r\")\n",
    "reg_exp_pattern = file.read()\n",
    "reg_exp__pattern=ast.literal_eval(reg_exp_pattern)\n",
    "file.close\n",
    "print(reg_exp__pattern)\n",
    "\n",
    "\n",
    "dic_spotwords={}\n",
    "ListKey = list_key.split()\n",
    "print(ListKey)\n",
    "for word in ListKey:\n",
    "    dup_synonyms=[]\n",
    "    for synonym in wordnet.synsets(word):\n",
    "        for lemma in synonym.lemmas():\n",
    "            dup_synonyms.append(lemma.name())\n",
    "\n",
    "    unique_synonyms=[]\n",
    "    for synonym in dup_synonyms:\n",
    "        if synonym not in unique_synonyms:\n",
    "            unique_synonyms.append(synonym)\n",
    "    dic_spotwords[word]=unique_synonyms\n",
    "print(dic_spotwords)\n",
    "# I don't think we use these regular expressions\n",
    "#remove reg exp dictionary #vishnu\n",
    "\n",
    "#These need to be greatly ezxpanded and turnedinto a file for reading\n",
    "#vishnu removed those disctonary (standard responses)\n",
    "\n",
    "# This section loads the necessary information for the \"alternate_phrase\" function below. The csv contains:\n",
    "# (1) RE patterns for creating specific chunks from using POS labels\n",
    "# (2) Identifying the portion of the user input that will be \"turned\" back to them in the response\n",
    "# (3) The framework response phrases that the portion identifiied in (2) will be married to\n",
    "\n",
    "input_terms_df = pd.read_csv('input_chunk_terms.csv', \n",
    "                             dtype={'capture_expression':'str', \n",
    "                                     'insertion':'bool', \n",
    "                                     'response_template':'str'}\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used by the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code written by Vishnu Lasya Marthala and edited by David Swanson \n",
    "#vishnu  \n",
    "def standard_phrase(user_keyword):\n",
    "    \n",
    "    #print('into the loop')\n",
    "    for key_r,value_r in standard_response.items():\n",
    "        #print(key_r)\n",
    "        if(user_keyword ==  key_r ):\n",
    "            #print('userkeyword:',user_keyword,key_r)\n",
    "            return(value_r)\n",
    "    return(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove #vishnu \n",
    "#def standard_phrase(text):\n",
    "    #for token in text:    \n",
    "    # for key, value in dic_spotwords.items():\n",
    "           # if text in value:\n",
    "                #print('text is', text)\n",
    "              #  for key_r,value_r in standardResponses.items():\n",
    "                   # print(key_r)\n",
    "                   # if(key == key_r):\n",
    "                      #  print('went into final if')\n",
    "                      #  return(value_r)\n",
    "           # return(\"\")\n",
    "#vishnu\n",
    "def regex_pattern(sentence):\n",
    "    for key,value in reg_exp__pattern.items():\n",
    "        m=re.search(key,sentence.lower())\n",
    "        ta=None\n",
    "        if(m):\n",
    "            s=random.choice(value)\n",
    "            for rpy in s:\n",
    "                if '{0}' in s:\n",
    "                    ta=m.group(1)\n",
    "                    return(s.format(ta))\n",
    "    return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method for parsing user input and generating a response - code written by Robb Dunlap\n",
    "\n",
    "def alternate_phrase(user_sentence):\n",
    "    new_sentence = \"\"\n",
    "    new_word = \"\"\n",
    "    for word in user_sentence.split():\n",
    "        for key, root in contractions.items():\n",
    "            if key == word:\n",
    "                word = root\n",
    "        new_word = word+\" \"\n",
    "        new_word += \" \"\n",
    "        # Had to put the words back together as a string instead of as a list because the two separate base words on the contraction\n",
    "        # end up being one entry in a list. So instead, reassemble the sentence as a string and split it again to be sure to get\n",
    "        # individual words\n",
    "        new_sentence += new_word\n",
    "\n",
    "    # need to strip off extra space added to the end of the sentence\n",
    "    new_sentence = new_sentence[:-2]\n",
    "\n",
    "    # create a list of the sentence words in order\n",
    "    split_sent = new_sentence.split()\n",
    "    sent_words_wo_punct = [w.strip(punctuation) for w in split_sent]\n",
    "\n",
    "\n",
    "    tokens_tag = treebankTagger.tag(sent_words_wo_punct)\n",
    "\n",
    "    index_in_df = -1\n",
    "    # for statement in chunk_list:\n",
    "    for statement in input_terms_df['capture_expression']:\n",
    "        pattern_found = 0\n",
    "        index_in_df+=1\n",
    "        phrase_pattern = statement\n",
    "        chunker = RegexpParser(phrase_pattern)\n",
    "        if index_in_df<2:\n",
    "            output = chunker.parse(tokens_tag)\n",
    "        else:\n",
    "            output = chunker.parse(output)\n",
    "\n",
    "        key_word = []\n",
    "        for item in output:\n",
    "            if isinstance(item, nltk.tree.Tree):\n",
    "                pattern_found = 1\n",
    "                regex_desired_word = re.compile(input_terms_df.iloc[index_in_df,3])\n",
    "                for thing in item:\n",
    "                    if re.match(regex_desired_word, thing[1]):     \n",
    "                        key_word = thing[0]\n",
    "        if pattern_found == 1:\n",
    "            break\n",
    "\n",
    "    # the below section captures the desired portion of the sentence so it can be flipped into the response\n",
    "\n",
    "    position_in_sent_word = 0\n",
    "    counter = 0\n",
    "    for element in sent_words_wo_punct:\n",
    "        if key_word == element:\n",
    "            counter = position_in_sent_word\n",
    "        position_in_sent_word += 1\n",
    "\n",
    "    offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "    turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "\n",
    "    # This section appends or inserts the flipped portion of the input text to the appropriate template \n",
    "    if pattern_found == 1:\n",
    "        if not input_terms_df.iloc[index_in_df,1]:\n",
    "            offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "            turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "            temp_holder = \"\"\n",
    "            separator = \" \"\n",
    "            temp_holder = separator.join(turned_portion_of_sent)\n",
    "            question_mark = \"?\"\n",
    "            temp_holder.join(question_mark)\n",
    "            eliza_response = input_terms_df.iloc[index_in_df,2]+\" \"+temp_holder+\"?\"\n",
    "        else:\n",
    "            offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "            turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "            temp_holder = \"\"\n",
    "            separator = \" \"\n",
    "            temp_holder = separator.join(turned_portion_of_sent)\n",
    "            turned_portion_of_sent = str(temp_holder)\n",
    "            eliza_response = input_terms_df.iloc[index_in_df,2].replace(\"XXYYMM\",turned_portion_of_sent)\n",
    "        return eliza_response    \n",
    "\n",
    "    else:\n",
    "        elize_response = \"\"\n",
    "        return elize_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliza welcomes the patient, sets the tone for the conversation, and establishes an end of session codeword \"quit.\"\n",
    "#### We decided that Eliza could asked for the patient's name which is used to tag the patient's inputs lines as well as a salutation.\n",
    "Code written by David Swanson and Vishnu Lasya Marthala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To use Spacy en_core_web_sm** I had to load add the Spacy module to my Conda environment and then I also had to use the following code in a terminal in my environment to add the library: &emsp; `conda install -c conda-forge spacy-model-en_core_web_sm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code is a loop that\n",
    "1. Tokenizes the sentances in the response\n",
    "2. If the patient talks too long (more than two sentences) Eliza asks in several ways for the patient to slow down \n",
    "3. Tokenizes the words of each sentence to be sent to four functions in sequence.\n",
    "    1. Has the patient made a statement that is dangerous to themselves of the theoropist (aka emergency words)? \n",
    "                     - If so, instruct the patient to seek help and end the session \n",
    "    2. Can Eliza respond using spotted words?\n",
    "    3. Can a sophisticated regular expression algorithm make a response?\n",
    "    4. If all these fail, Eliza assumes the answer was gibberish and randomly selects a response for the patient to talk more.\n",
    "    \n",
    "Code written by: David Swanson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: I don't want you to think of these sessions as therapy but rather as an opportunity for self-reflection and growth.\n",
      "Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"\n",
      "\n",
      "Eliza: How can I address you?\n",
      "\n",
      "Patient: I am Stella\n",
      "(Stella,)\n",
      "[Stella]\n",
      "Eliza: Hello Stella. How are you feeling today?\n",
      "\n",
      "Stella: can you help me\n",
      "Eliza1:  I am here to help you with your emotions and thought process.\n",
      "\n",
      "Stella: okay. i want to know more\n",
      "Eliza4:  That is interesting, go on.\n",
      "\n",
      "Eliza4:  Tell me more.\n",
      "\n",
      "\n",
      "Stella: okay. i am sad\n",
      "Eliza4:  Please tell me more.\n",
      "\n",
      "Eliza2:  Why do you think you are sad?\n",
      "\n",
      "Stella: i am not sure\n",
      "Eliza2:  Why do you think you are not sure?\n",
      "\n",
      "Stella: but, i am positive\n",
      "Eliza1:  Cool! Go ahead.\n",
      "\n",
      "Stella: i am having wonderful time\n",
      "Eliza2:  Why do you think you are having wonderful time?\n",
      "\n",
      "Stella: i am not sure\n",
      "Eliza2:  Why do you think you are not sure?\n",
      "\n",
      "Stella: i crave to have more in life\n",
      "Eliza2:  Why do you crave have more in life?\n",
      "\n",
      "Stella: do you care\n",
      "Eliza3:  Can you expand on why you care?\n",
      "\n",
      "Stella: cheers\n",
      "Eliza4:  Tell me more.\n",
      "\n",
      "\n",
      "Stella: okay. bye\n",
      "Eliza4:  I'm not sure I'm following you, please explain. \n",
      "\n",
      "Eliza1:  Bye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Eliza: Hello! My name is Eliza.  Welcome to my practice.\")\n",
    "print(\"Eliza: I don't want you to think of these sessions as therapy but rather as an opportunity for self-reflection and growth.\")\n",
    "print('Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"')\n",
    "print(\"\")\n",
    "\n",
    "# Get the patient's name\n",
    "print(\"Eliza: How can I address you?\") #vishnu\n",
    "print(\"\")\n",
    "user_input = input(\"Patient: \")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(user_input)  \n",
    "print(doc.ents )\n",
    "usernames=[e for e in doc.ents if e.label_ == 'PERSON']\n",
    "\n",
    "\n",
    "if doc.ents==():\n",
    "    patientName = \"Patient: \"\n",
    "    print(\"Eliza: That's ok, we don't need to use names. So how are you feeling today?\")\n",
    "else:\n",
    "    patientName=usernames[0].text\n",
    "    print('Eliza: Hello '+ patientName +'. '+\"How are you feeling today?\")\n",
    "\n",
    "    \n",
    "# Entering while loop for continuing discussion with the patient\n",
    "\n",
    "loopAgain = True\n",
    "\n",
    "# Top of the loop\n",
    "while (loopAgain == True):\n",
    "    \n",
    "    # Get the patient's input \n",
    "    print(\"\")\n",
    "    user_input = input(patientName+\": \")\n",
    "    \n",
    "# First check the patient's response to see if they want to end the session.\n",
    "    if (user_input.lower() == \"quit\"):\n",
    "        print(\"Eliza: I hope our session was helpful.  Goodbye\",patientName)\n",
    "        loopAgain = False\n",
    "        break\n",
    "        \n",
    "# Start a counter to cycle through some random responses when the patients drones on\n",
    "    lengthyReponse = 0\n",
    "    \n",
    "# Separate the response into individual sentences\n",
    "    PatientResponse = nltk.sent_tokenize(user_input)\n",
    "    # print(PatientResponse)\n",
    "    \n",
    "# Send the patient's response word by word to check for emergency words\n",
    "# Code written by Anh \"Tim\" Hien Bach\n",
    "\n",
    "    HaltFlag = 0\n",
    "    for sentences in PatientResponse:\n",
    "        PatientWords = nltk.word_tokenize(sentences)\n",
    "        for words in PatientWords: \n",
    "            for EmWord in emergency_words:\n",
    "                # If any emergency word is found, return with a standard cautionary response\n",
    "                if re.search(EmWord, words):\n",
    "                    print(patientName,\": \", emergency_response)\n",
    "                    loopAgain = False\n",
    "                    break\n",
    "            if loopAgain == False:\n",
    "                break\n",
    "        if loopAgain == False:\n",
    "            break\n",
    "\n",
    "# If the response is more than 2 sentences\n",
    "    if loopAgain == True and len(PatientResponse)>2:\n",
    "        lengthyReponse = lengthyReponse + 1\n",
    "\n",
    "# Cycle through two responses if the patient inputs 3 sentences \n",
    "        if(len(PatientResponse)==3): \n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: Let's go slower and take things one at a time. What is on your mind?\")\n",
    "            if(lengthyReponse > 1):\n",
    "                print(\"Eliza: Again, slow down. Try again.\")\n",
    "                lengthy_response = 0\n",
    "                \n",
    "# Cycle through three three responses if the patient inputs more than 3 sentences \n",
    "        else: \n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: Whoa, that's a lot to unpack. Let's try again with shorter thoughts.\")\n",
    "            if(lengthyReponse == 2):\n",
    "                print(\"Eliza: Again, slow down. Try again.\")\n",
    "            if(lengthyReponse > 2):\n",
    "                print(\"Eliza: You have alot on your mind. Let's start again with your first thought.\")\n",
    "                lengthyReponse = 0\n",
    "\n",
    "# If the patient gives one or two sentence response, address each            \n",
    "    else:\n",
    "        if loopAgain == False:\n",
    "            break\n",
    "        for sentence in PatientResponse:\n",
    "            \n",
    "            # We don't weant contractions in our evaluation of the patient's input so we expand them \n",
    "            newSentence = \"\"\n",
    "            for word in sentence.split():\n",
    "                for key, root in contractions.items():\n",
    "                    if key == word:\n",
    "                        word = root\n",
    "                    elif key == word.lower():\n",
    "                        word = root\n",
    "            \n",
    "            # Tokenize the updated sentence into words\n",
    "            tokenizer = RegexpTokenizer (r'\\w+')\n",
    "            words_only = tokenizer.tokenize(sentence)\n",
    "            #print(words_only)\n",
    "            # Debugging Code\n",
    "            # print (\"The total number of words in sentence\", i, \"of the user's input is:\", len(words_only)) \n",
    "            # print (words_only)\n",
    "    \n",
    "            # Set ElizaResponse to NULL\n",
    "            ElizaResponse = \"\"\n",
    "            \n",
    "            #vishnu\n",
    "            # Send the patient's sentence to a word spotter routine \n",
    "            match =0 \n",
    "            for token in words_only:\n",
    "                for key, value in dic_spotwords.items():\n",
    "                    #print('key is : ', key)\n",
    "                    if token in value:\n",
    "                        #print(token)\n",
    "                        match=1\n",
    "                        #if ElizaResponse != \"\":\n",
    "                        ElizaResponse = standard_phrase(key)\n",
    "                        print(\"Eliza1: \", ElizaResponse)\n",
    "                        break\n",
    "                        \n",
    "            #vishnu     \n",
    "            if(match!=1):\n",
    "                ElizaResponse = regex_pattern(sentence)\n",
    "                if(ElizaResponse != \"\"):\n",
    "                    match=2\n",
    "                    \n",
    "                    print(\"Eliza2: \", ElizaResponse)\n",
    "                    break\n",
    "                               \n",
    "                \n",
    "            # If the standard phrase routine fails to provide a response send to the alternate function \n",
    "            # We use the untokenized\n",
    "            if(match!=1 and match!=2): #vishnu\n",
    "                ElizaResponse = alternate_phrase(PatientResponse[0])\n",
    "                if ElizaResponse != \"\":\n",
    "                    match=3\n",
    "                    print(\"Eliza3: \", ElizaResponse)\n",
    "                    break\n",
    "                    \n",
    "            # Those don't provide a response, it is gibberish so send a random response\n",
    "            if(match!=1 and match !=2 and match!=3): #vishnu\n",
    "                print(\"Eliza4: \", random.choice(nonspecific_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am sad']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PatientResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
